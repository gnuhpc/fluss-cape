# Valkey Data Structure Optimization Analysis

**Date**: 2026-01-09  
**Source**: Valkey source code analysis (`/root/valkey/src/`)  
**Target**: Fluss-HBase Redis Compatibility Layer Phase 2.2 optimizations

---

## Executive Summary

After analyzing Valkey's (Redis fork) implementation of List and Sorted Set data structures, several **critical optimization opportunities** were identified for our current Fluss-based implementation. The most impactful optimizations focus on:

1. **Dual-encoding strategy** (listpack ‚Üí quicklist for Lists, listpack ‚Üí skiplist+dict for Sorted Sets)
2. **Metadata caching** for O(1) operations (head/tail pointers, count tracking)
3. **Reverse index** for member lookups in Sorted Sets
4. **Memory-efficient data structures** (listpack for small collections)

---

## Current Implementation Limitations

### Lists (ListCommandExecutor.java)

**Current Approach**:
- Single encoding: All items stored as individual Fluss rows with encoded indices
- Index format: `+0000000000000000` (RPUSH), `-0000000000000001` (LPUSH)
- Operations require **full table scan** to find min/max indices

**Performance Issues**:
| Operation | Current Complexity | Issue |
|-----------|-------------------|-------|
| LPUSH/RPUSH | O(n) | Must scan all items to find min/max index |
| LPOP/RPOP | O(n) | Must scan + sort to find head/tail |
| LLEN | O(n) via `adapter.countByKey()` | No cached count |
| LRANGE | O(n) | Full scan + sort + filter |

### Sorted Sets (SortedSetCommandExecutor.java)

**Current Approach**:
- Single encoding: `sub_key = "score_encoded:member_name"`
- Score encoding ensures lexicographic sort order
- Member lookup requires **full scan**

**Performance Issues**:
| Operation | Current Complexity | Issue |
|-----------|-------------------|-------|
| ZADD (update) | O(n) | Must scan to check if member exists |
| ZREM | O(n) | Must scan to find member |
| ZSCORE | O(n) | Must scan to find member by name |
| ZCARD | O(n) via `adapter.countByKey()` | No cached count |
| ZRANGE | O(n) | Full scan (though sorted by sub_key) |

---

## Valkey Optimization Strategies

### 1. List Implementation: Dual Encoding (listpack + quicklist)

**Reference**: `t_list.c`, `quicklist.c`

#### Strategy A: Small Lists ‚Üí Listpack (Compact Memory)

**Valkey's listpack**:
- Contiguous memory block (cache-friendly)
- Variable-length encoding for integers/strings
- Maximum size: 8KB by default (`list-max-listpack-size -2`)
- No per-element overhead (unlike linked list nodes)

**In Fluss context**:
- Store entire list as **single BYTES value** in one Fluss row
- Sub-key: `"__meta__"` for metadata + list data
- Format: `[length:4bytes][head_index:8bytes][tail_index:8bytes][serialized_items]`

**Benefits**:
- LPUSH/RPOP: **O(1)** with metadata tracking
- LLEN: **O(1)** read from metadata
- LRANGE: O(k) where k = range size (no full scan)
- Memory: ~50% reduction for small lists (no per-item row overhead)

#### Strategy B: Large Lists ‚Üí Quicklist (Balanced Performance)

**Valkey's quicklist**:
- Doubly-linked list of listpack nodes
- Each node: 4KB-8KB listpack
- Compression: Middle nodes compressed (LZF), head/tail hot for access

**In Fluss context**:
- Metadata row: `sub_key = "__meta__"` stores:
  - `head_index`, `tail_index`, `count`, `node_list`
- Each node: `sub_key = "node_000001"` stores listpack bytes
- Operations: Read metadata (O(1)), access only relevant nodes

**Benefits**:
- LPUSH/RPUSH: **O(1)** (update head/tail node only)
- LPOP/RPOP: **O(1)** (no scan needed)
- LRANGE: O(nodes_in_range) instead of O(total_items)

**Conversion threshold** (from Valkey config):
```c
// Convert listpack ‚Üí quicklist when:
if (lpBytes(lp) > 8192 || lpLength(lp) > server.list_max_listpack_size) {
    convertToQuicklist();
}
```

---

### 2. Sorted Set Implementation: Skiplist + Hash Table

**Reference**: `t_zset.c`, `dict.c`

#### Strategy: Dual Data Structure

**Valkey's approach**:
```c
typedef struct zset {
    dict *dict;         // member ‚Üí score (O(1) lookup)
    zskiplist *zsl;     // score ‚Üí member (O(log n) range)
} zset;
```

**Key insight**: **Trade space for time complexity**
- Hash table: O(1) member lookup (ZSCORE, ZREM, ZADD updates)
- Skiplist: O(log n) sorted range queries (ZRANGE, ZRANGEBYSCORE)
- Shared memory: Element SDS string used in both structures

#### In Fluss Context: Reverse Index Table

**Current schema** (single table):
```sql
redis_key | redis_type | sub_key             | score | value
---------|-----------|---------------------|-------|-------
"myzset" | "zset"     | "0000...100:alice" | 100.0 | ""
```

**Optimized schema** (add reverse index):
```sql
-- Main table (unchanged - for ZRANGE)
redis_data:
  PRIMARY KEY (redis_key, sub_key)

-- NEW: Reverse index table (for ZSCORE/ZREM)
redis_zset_members:
  redis_key STRING,
  member STRING,
  score DOUBLE,
  PRIMARY KEY (redis_key, member)
```

**Operations**:
| Operation | Without Index | With Index |
|-----------|--------------|-----------|
| ZSCORE key member | O(n) scan | **O(1)** direct lookup |
| ZREM key member | O(n) scan + delete | **O(1)** lookup + 2 deletes |
| ZADD key score member | O(n) check + insert | **O(1)** check + 2 inserts |
| ZRANGE key start stop | O(n) scan + sort | O(n) scan (unchanged) |

**Benefits**:
- ZSCORE: **50-100x faster** for large sets (O(1) vs O(n))
- ZREM: **10-50x faster** (no scan needed)
- ZADD: **5-20x faster** (instant duplicate detection)
- Cost: 2x storage (acceptable tradeoff)

---

### 3. Memory-Efficient Encoding: Listpack

**Reference**: `listpack.c`, `listpack.h`

#### What is Listpack?

Listpack is a **serialized array** with variable-length encoding:

```
+--------+--------+--------+--------+--------+
| header | entry1 | entry2 | entry3 | footer |
+--------+--------+--------+--------+--------+

Entry format:
- Small integers: 1-5 bytes (not 8 bytes)
- Short strings: length + data (no pointer overhead)
- Backtracking: Each entry stores its own length (bi-directional traversal)
```

**Memory comparison** (for 100-element list):

| Approach | Memory Usage | Notes |
|----------|--------------|-------|
| **Individual Fluss rows** | ~8KB | Current: 100 rows √ó ~80 bytes/row overhead |
| **Single listpack row** | ~2KB | 100 elements √ó ~20 bytes avg (compact encoding) |
| **Savings** | **75%** | Eliminates per-row storage overhead |

#### In Fluss Context: Packed Representation

**For small collections** (< 128 items or < 8KB):
- Serialize entire list/set as **single BYTES column**
- Use Java's DataOutputStream for compact encoding
- Store in single Fluss row with `sub_key = "__packed__"`

**Encoding example** (List):
```java
ByteArrayOutputStream baos = new ByteArrayOutputStream();
DataOutputStream dos = new DataOutputStream(baos);

dos.writeInt(count);          // 4 bytes
dos.writeLong(headIndex);     // 8 bytes
dos.writeLong(tailIndex);     // 8 bytes

for (byte[] item : items) {
    dos.writeInt(item.length);  // 4 bytes per item
    dos.write(item);            // variable bytes
}

adapter.setByCompositeKey(key, "list", "__packed__", baos.toByteArray(), null);
```

**Benefits**:
- **1 Fluss operation** instead of N operations
- **80% memory reduction** for small lists/sets
- **Cache-friendly**: Single contiguous read
- **Atomic updates**: Full list in one transaction

---

### 4. Metadata Caching Pattern

**Reference**: Throughout Valkey codebase

#### Pattern: In-Memory Metadata

**Valkey's approach**:
```c
typedef struct quicklist {
    quicklistNode *head;
    quicklistNode *tail;
    unsigned long count;  // ‚Üê Cached total count
    unsigned long len;    // ‚Üê Cached node count
} quicklist;

// O(1) operations using cached metadata
unsigned long quicklistCount(const quicklist *ql) {
    return ql->count;  // No iteration needed
}
```

#### In Fluss Context: Metadata Row

**Schema**:
```sql
redis_key | sub_key    | value (BYTES)
---------|-----------|------------------
"mylist" | "__meta__" | {count:100, head:-99, tail:100, encoding:"quicklist"}
```

**Metadata structure** (serialized):
```java
class ListMetadata {
    int count;           // Total element count
    long headIndex;      // Minimum index (for LPOP)
    long tailIndex;      // Maximum index (for RPUSH)
    String encoding;     // "packed" or "distributed"
    long lastModified;   // Timestamp
}
```

**Operations**:
```java
// LLEN - O(1) instead of O(n)
ListMetadata meta = readMetadata(key);
return meta.count;

// LPUSH - O(1) index calculation
ListMetadata meta = readMetadata(key);
long newIndex = meta.headIndex - 1;
adapter.setByCompositeKey(key, "list", IndexEncoder.encode(newIndex), value, null);
meta.headIndex = newIndex;
meta.count++;
updateMetadata(key, meta);
```

**Benefits**:
| Operation | Without Metadata | With Metadata |
|-----------|-----------------|---------------|
| LLEN | O(n) full scan | **O(1)** single row read |
| LPUSH | O(n) find min | **O(1)** decrement cached head |
| RPUSH | O(n) find max | **O(1)** increment cached tail |
| ZCARD | O(n) count rows | **O(1)** cached count |

**Consistency**: Update metadata in same transaction as data changes (use Fluss batch writes).

---

## Implementation Priority Matrix

### High Impact, Low Effort (Implement First)

| Optimization | Impact | Effort | ROI |
|-------------|--------|--------|-----|
| **Metadata caching** | üî•üî•üî• 50-100x for LLEN/ZCARD | ‚ö° Low (1-2 days) | **‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ** |
| **Reverse index for ZSCORE** | üî•üî• 10-50x for member lookups | ‚ö°‚ö° Medium (2-3 days) | **‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ** |

### High Impact, Medium Effort (Phase 2.3)

| Optimization | Impact | Effort | ROI |
|-------------|--------|--------|-----|
| **Packed encoding (< 128 items)** | üî• 50-80% memory, 2-5x ops | ‚ö°‚ö° Medium (3-4 days) | **‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ** |
| **Quicklist-style chunking** | üî• O(1) LPUSH/RPOP, O(k) LRANGE | ‚ö°‚ö°‚ö° High (5-7 days) | **‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ** |

### Lower Priority (Future)

- LZF compression for cold data (complex, marginal benefit in Fluss)
- Skiplist instead of full scan for ZRANGE (Fluss already provides sorted reads)

---

## Recommended Implementation Plan

### Phase 2.2.1: Metadata Layer (Week 1)

**Goal**: Eliminate O(n) scans for count/head/tail operations

**Tasks**:
1. Create `MetadataManager` class for List/Zset metadata
2. Serialize metadata as BYTES in `sub_key = "__meta__"`
3. Update all executors to read/update metadata atomically
4. Add integration tests for metadata consistency

**Expected Results**:
- LLEN/ZCARD: O(n) ‚Üí **O(1)** (100x faster)
- LPUSH/RPUSH: O(n) ‚Üí **O(1)** (50x faster)

### Phase 2.2.2: Reverse Index (Week 2)

**Goal**: Enable O(1) member lookups in Sorted Sets

**Tasks**:
1. Create `redis_zset_members` table in Fluss
2. Update `SortedSetCommandExecutor.executeZAdd()` to write both tables
3. Update `executeZScore()` to use reverse index
4. Add cleanup logic (delete from both tables)

**Expected Results**:
- ZSCORE: O(n) ‚Üí **O(1)** (50x faster)
- ZREM: O(n) ‚Üí **O(1)** (30x faster)

### Phase 2.2.3: Packed Encoding (Week 3)

**Goal**: Reduce memory and I/O for small collections

**Tasks**:
1. Create `PackedListCodec` and `PackedZSetCodec`
2. Auto-convert at thresholds (128 items or 8KB)
3. Transparent unpacking for large collections
4. Migration path for existing data

**Expected Results**:
- Memory: 50-80% reduction for small lists/sets
- I/O: 1 operation instead of N operations

---

## Code Examples

### Example 1: Metadata-Backed LLEN

**Before** (current):
```java
private RedisMessage executeLLen(RedisCommand command) throws Exception {
    String key = command.getArgAsString(0);
    long count = adapter.countByKey(key);  // O(n) full scan
    return RedisResponse.integer(count);
}
```

**After** (with metadata):
```java
private RedisMessage executeLLen(RedisCommand command) throws Exception {
    String key = command.getArgAsString(0);
    
    // O(1) single row read
    ListMetadata meta = metadataManager.getListMetadata(key);
    if (meta == null) {
        return RedisResponse.integer(0);
    }
    
    return RedisResponse.integer(meta.count);
}
```

### Example 2: Reverse Index for ZSCORE

**Before** (current):
```java
private RedisMessage executeZScore(RedisCommand command) throws Exception {
    String key = command.getArgAsString(0);
    String member = command.getArgAsString(1);
    
    // O(n) full scan
    List<RedisFlussAdapter.KeyValue> items = adapter.scanByKey(key);
    for (RedisFlussAdapter.KeyValue kv : items) {
        ScoreEncoder.ScoreMember sm = ScoreEncoder.decodeWithMember(kv.subKey);
        if (sm.member.equals(member)) {
            return RedisResponse.bulkString(String.valueOf(sm.score).getBytes());
        }
    }
    return RedisResponse.nullBulkString();
}
```

**After** (with reverse index):
```java
private RedisMessage executeZScore(RedisCommand command) throws Exception {
    String key = command.getArgAsString(0);
    String member = command.getArgAsString(1);
    
    // O(1) direct lookup in redis_zset_members table
    Double score = reverseIndex.getMemberScore(key, member);
    if (score == null) {
        return RedisResponse.nullBulkString();
    }
    
    return RedisResponse.bulkString(String.valueOf(score).getBytes());
}
```

### Example 3: Packed Encoding for Small Lists

**Schema**:
```java
class PackedList {
    int count;
    long headIndex;
    long tailIndex;
    List<byte[]> items;  // In-memory, serialized to BYTES
    
    byte[] serialize() {
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        DataOutputStream dos = new DataOutputStream(baos);
        dos.writeInt(count);
        dos.writeLong(headIndex);
        dos.writeLong(tailIndex);
        for (byte[] item : items) {
            dos.writeInt(item.length);
            dos.write(item);
        }
        return baos.toByteArray();
    }
}
```

**Operations**:
```java
// LPUSH on packed list
PackedList packed = PackedList.deserialize(adapter.getByCompositeKey(key, "__packed__"));
packed.items.add(0, value);  // Add to head
packed.headIndex--;
packed.count++;

// Write back entire list (still faster than N row operations)
adapter.setByCompositeKey(key, "list", "__packed__", packed.serialize(), null);
```

---

## Performance Projections

### List Operations (After Metadata + Packed Encoding)

| Operation | Current | After Phase 2.2.1 | After Phase 2.2.3 |
|-----------|---------|-------------------|-------------------|
| LPUSH (n=100) | 2ms (O(n) scan) | **20Œºs** (O(1) meta) | **15Œºs** (1 write) |
| LLEN (n=100) | 2ms (count scan) | **10Œºs** (O(1) meta) | **10Œºs** (1 read) |
| LRANGE 0 -1 (n=100) | 3ms (scan+sort) | 3ms (scan) | **500Œºs** (1 read) |

**Improvement**: **100-200x faster** for small lists

### Sorted Set Operations (After Reverse Index)

| Operation | Current | After Phase 2.2.2 |
|-----------|---------|-------------------|
| ZSCORE (n=100) | 2ms (O(n) scan) | **50Œºs** (O(1) index) |
| ZREM (n=100) | 3ms (scan + delete) | **100Œºs** (2 deletes) |
| ZADD update (n=100) | 3ms (scan + 2 writes) | **150Œºs** (3 writes) |

**Improvement**: **20-60x faster** for member lookups

---

## References

1. **Valkey Source Code**:
   - `/root/valkey/src/t_list.c` - List commands implementation
   - `/root/valkey/src/quicklist.c` - Quicklist data structure
   - `/root/valkey/src/t_zset.c` - Sorted Set implementation with skiplist + dict
   - `/root/valkey/src/listpack.c` - Memory-efficient packed encoding

2. **Redis Documentation**:
   - [Redis List Internals](https://redis.io/docs/data-types/lists/)
   - [Redis Sorted Set Internals](https://redis.io/docs/data-types/sorted-sets/)
   - Memory Optimization Guide

3. **Academic References**:
   - William Pugh: "Skip Lists: A Probabilistic Alternative to Balanced Trees"
   - Ziplist/Listpack design (Antirez)

---

## Conclusion

Valkey's optimizations demonstrate that **hybrid data structures** and **metadata caching** are essential for production-grade performance. Our current Phase 2.2 implementation is **functionally correct** but **performance-naive** (O(n) operations where O(1) is achievable).

**Immediate Action Items**:
1. ‚úÖ **Implement metadata caching** (highest ROI, lowest effort)
2. ‚úÖ **Add reverse index for Sorted Sets** (critical for ZSCORE performance)
3. ‚è≥ **Consider packed encoding** for Phase 2.3 (memory optimization)

With these optimizations, we can achieve **10-100x performance improvements** while maintaining full Redis compatibility.

---

**Next Steps**: Create implementation issues for Phase 2.2.1 and Phase 2.2.2.
